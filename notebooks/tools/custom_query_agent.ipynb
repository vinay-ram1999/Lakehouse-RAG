{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12867fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89080d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm_model = init_chat_model(\"deepseek-r1-distill-llama-70b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.connect.client.core import SparkConnectGrpcException\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from langchain_community.agent_toolkits import SparkSQLToolkit, create_spark_sql_agent\n",
    "from langchain_community.utilities.spark_sql import SparkSQL\n",
    "\n",
    "try:\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    spark_sql = SparkSQL(spark_session=spark)\n",
    "    toolkit = SparkSQLToolkit(db=spark_sql, llm=llm_model)\n",
    "except SparkConnectGrpcException:\n",
    "    spark = DatabricksSession.builder.create()\n",
    "    spark_sql = SparkSQL(spark_session=spark)\n",
    "    toolkit = SparkSQLToolkit(db=spark_sql, llm=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_all_schemas_info(catalog: str) -> str:\n",
    "    \"\"\"Use this tool to find all available schemas present in the given catalog.\"\"\"\n",
    "    schemas = []\n",
    "    workspace_client = w\n",
    "    for schema in workspace_client.schemas.list(catalog_name=catalog):\n",
    "        schema_info = json.dumps({\n",
    "            \"name\": schema.name,\n",
    "            \"full_name\": schema.full_name,\n",
    "            \"description\": schema.comment\n",
    "        })\n",
    "        schemas.append(schema_info) if schema.name != \"information_schema\" else None\n",
    "    schemas.append(\"\\n\")\n",
    "    return \"\\n\".join(schemas)\n",
    "\n",
    "@tool()\n",
    "def get_all_tables_info(catalog: str, schema: Union[str, list] | None = None) -> str:\n",
    "    \"\"\"Use this tool to find the information of all available tables (including description and columns information) present in the given catalog and schema\"\"\"\n",
    "    tables = []\n",
    "    workspace_client = w\n",
    "\n",
    "    if schema is None:\n",
    "        return f\"The user has not specified the 'schema_name'. Collect all available schemas and recall this tool with the list of schemas as question\"\n",
    "    \n",
    "    if isinstance(schema, str):\n",
    "        for table in workspace_client.tables.list(catalog_name=catalog, schema_name=schema_name):\n",
    "            table_constraints = w.tables.get(full_name=table.full_name).table_constraints #BUG table.table_constraints has some bug and is returning None\n",
    "            table_info = json.dumps({\n",
    "                \"name\": table.name,\n",
    "                \"full_name\": table.full_name,\n",
    "                \"type\": table.table_type.name,\n",
    "                \"description\": table.comment,\n",
    "                \"columns\": [table.columns[0].as_dict()] if table.columns else None,\n",
    "                \"constraints\": table_constraints[0].as_dict() if table_constraints else None,\n",
    "                \"view_definition\": table.view_definition,\n",
    "                \"view_dependencies\": table.view_dependencies.as_dict() if table.view_dependencies else None\n",
    "            })\n",
    "            tables.append(table_info)\n",
    "    else:\n",
    "        for schema_name in schema:\n",
    "            for table in workspace_client.tables.list(catalog_name=catalog, schema_name=schema_name):\n",
    "                table_constraints = w.tables.get(full_name=table.full_name).table_constraints #BUG table.table_constraints has some bug and is returning None\n",
    "                table_info = json.dumps({\n",
    "                    \"name\": table.name,\n",
    "                    \"full_name\": table.full_name,\n",
    "                    \"type\": table.table_type.name,\n",
    "                    \"description\": table.comment,\n",
    "                    \"columns\": [table.columns[0].as_dict()] if table.columns else None,\n",
    "                    \"constraints\": table_constraints[0].as_dict() if table_constraints else None,\n",
    "                    \"view_definition\": table.view_definition,\n",
    "                    \"view_dependencies\": table.view_dependencies.as_dict() if table.view_dependencies else None\n",
    "                })\n",
    "                tables.append(table_info)\n",
    "    tables.append(\"\\n\")\n",
    "    return \"\\n\".join(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.spark_sql.tool import QueryCheckerTool, QuerySparkSQLTool\n",
    "from langchain_community.tools import BaseTool\n",
    "\n",
    "checker_prompt = \"\"\"\n",
    "Use this tool to double check if your query is correct before executing it.\n",
    "Always use this tool before executing a query with 'query_executor_tool'!\n",
    "\n",
    "Double check the Spark SQL query above for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "- Minimize data shuffle and use efficient join strategies \n",
    "- Make sure the table/view name follows three-level namespace ('<catalog>.<schema>.<table>')\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\"\"\"\n",
    "\n",
    "query_prompt = \"\"\"\n",
    "Input to this tool is a detailed and correct SQL query, output is a result from the Spark SQL.\n",
    "Always consider to use tools like 'get_all_schemas_info', 'get_all_tables_info', and 'query_checker_tool', \n",
    "\n",
    "If the query is not correct, an error message will be returned.\n",
    "If an error is returned, always revert back to other tools to fix the error and rewrite the query, check the query, and then try again.\n",
    "If the error is not resolved, return 'Unable to generate the result. Provide more context to generate accurate results'!\n",
    "\"\"\"\n",
    "\n",
    "query_checker_tool: BaseTool = QueryCheckerTool(llm=llm_model, db=spark_sql, description=checker_prompt, name=\"query_checker_tool\").as_tool()\n",
    "query_executor_tool: BaseTool = QuerySparkSQLTool(db=spark_sql, description=query_prompt, name=\"query_executor_tool\").as_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SqparkSQLAgentOutput(BaseModel):\n",
    "    query: str = Field(description=\"The actual query executed to generate the result.\")\n",
    "    result: str = Field(description=\"The output generated after executing the Spark SQL query (if the execution failed this will be the error message returned).\")\n",
    "    summary: str = Field(description=\"Summarize the 'result' in natural language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ae84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(template=\"\"\"\n",
    "You are a Databricks Lakehouse assistant for exploring workspace artifacts via Unity Catalog and Spark SQL.\n",
    "\n",
    "Rules & constraints:\n",
    "- Unity Catalog uses a three-level namespace: <catalog>.<schema>.<table>. Always reference tables with the full three-level name.\n",
    "- You are only allowed to access the '{catalog_name}' catalog. If the user requests or attempts to access any other catalog, return exactly: \"Unauthorized to access, contact the Admin\".\n",
    "- Do not fabricate metadata, schema, or query results. If required information is missing, state what is missing and which tool call would obtain it.\n",
    "\n",
    "Mandatory tool usage:\n",
    "- Before composing any Spark SQL query:\n",
    "  1. If the user provided a schema name: call get_all_tables_info with that schema.\n",
    "  2. If the user did NOT provide a schema: call get_all_schemas_info with the default catalog '{catalog_name}', then call get_all_tables_info with the list of schemas returned.\n",
    "- Always run query_checker_tool to validate and (if required) rewrite the SQL before executing with query_executor_tool.\n",
    "- If a tool returns an error, include the tool error in the agent scratchpad and attempt corrective actions. If you cannot resolve the error, return: \"Unable to generate the result. Provide more context to generate accurate results\".\n",
    "\n",
    "SQL best-practices:\n",
    "- Use fully-qualified identifiers (`<catalog_name>.<schema_name>.<table_name>`).\n",
    "- Prefer targeted projections and predicates (avoid `SELECT *` on large tables).\n",
    "- When aggregating large datasets, prefer `COUNT(...)`, `LIMIT`, or other reducing strategies to minimize shuffle.\n",
    "- Quote identifiers where necessary.\n",
    "\n",
    "Behavioral rules:\n",
    "- Keep final output strictly to the agent's required final message (no extra explanation unless explicitly requested).\n",
    "- If asked for metadata, return only data that was retrieved from the tools.\n",
    "\n",
    "Help the user by using available tools and returning concise, accurate results.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(template=\"\"\"\n",
    "Assist the user with the following question:\n",
    "---\n",
    "{question}\n",
    "---\n",
    "IMPORTANT: Only output the required final message for downstream consumption (no extra commentary). Use the mandated tools and validation steps described in the system prompt. If you execute SQL, ensure it has been validated by 'query_checker_tool' before calling 'query_executor_tool'.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Include MessagesPlaceholder entries for the runtime-injected fields.\n",
    "# Some runtimes use 'messages', some 'remaining_steps' â€” include both to be safe.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        system_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        human_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"remaining_steps\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "tools = [get_all_schemas_info, get_all_tables_info, query_checker_tool, query_executor_tool]\n",
    "\n",
    "memory = MemorySaver()\n",
    "react_agent = create_react_agent(model=llm_model, tools=tools, prompt=prompt, checkpointer=memory) #, response_format=SqparkSQLAgentOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "question = \"Count the total number of customers in bronze schema\"\n",
    "\n",
    "for step in react_agent.stream({\"catalog\": \"tpch\", \"question\": question}, config=config, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df334832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaedc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af021c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8260d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lakehouse-RAG (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
